{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benbezzina4/NLP2/blob/master/Colab_text_compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1TPJEXpvGYiQ"
      },
      "outputs": [],
      "source": [
        "# write your imports up here\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import io\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAyu8JGRLwMX",
        "outputId": "fa3014a7-8e42-4938-bc94-a7f5f219dc14"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVEURyKPLbmZ",
        "outputId": "524f7b09-7191-46a9-bfc7-af2f139949c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'"
      ],
      "metadata": {
        "id": "2YqdUwfxbB2H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HxCwtPjGYiW"
      },
      "source": [
        "# Text compression assignment\n",
        "\n",
        "You are to follow the instructions below and fill each cell as instructed.\n",
        "Once ready, submit this notebook on VLE with all the outputs included (run all your code and don't clear any output cells).\n",
        "Do not submit anything else apart from the notebook and do not use any extra data apart from what is provided.\n",
        "\n",
        "10% of the marks from this assignment are based on neatness.\n",
        "\n",
        "This assignment will carry 60% of the final mark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PAwLZiQGYiY"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "It is said that you can measure the intelligence of an AI from the amount it can compress a text without information loss.\n",
        "One way to think about this is that, the more a text is predictable, the more words we can leave out of it as we can guess the missing words.\n",
        "On the other hand, the more intelligent an AI is, the more it will find texts to be predictable and so the more words it can leave out and guess.\n",
        "This has led to a competition called the [Hutter Prize](http://prize.hutter1.net/) where the objective is to compress a given text as much as possible.\n",
        "The record for compressing a 1GB text file extracted from a Wikipedia snapshot is about 115MB.\n",
        "The main hurdle here is that the program used to decompress the file must be treated as part of the compressed file, meaning that the program itself must also be small.\n",
        "\n",
        "In this assignment, we're going to be doing something similar using a smaller text file and using neural language models to guess missing words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3alp-hPNGYiZ"
      },
      "source": [
        "## Data processing (10%)\n",
        "\n",
        "You have a train/dev/test split corpus of text from Wikipedia consisting of single sentences.\n",
        "Each sentence is on a separate line and each sentence has been tokenised for you such that tokens are space separated.\n",
        "This means that you only need to split by space to get the tokens.\n",
        "The text has all been lowercased as well.\n",
        "The objective here is to be able to compress the text losslessly, meaning that it can be decompressed back to the original string:\n",
        "\n",
        "$$\\text{decompress}(\\text{compress}(t)) = t$$\n",
        "\n",
        "Do not do any further pre-processing on the text (such as stemming) as it may result in unrecoverable information loss.\n",
        "The test set is what we will be compressing and will not be processed at all as it will be treated as a single big string by the compression/decompression algorithms.\n",
        "\n",
        "Do the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWU5y2gcGYia"
      },
      "source": [
        "Load the train set and dev set text files into a list of sentences where each sentence is tokenised (by splitting by space)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3cGd_KzRGYib"
      },
      "outputs": [],
      "source": [
        "def read_sentences_from_file(path_to_file):\n",
        "    lines = []\n",
        "    with io.open(path_to_file, mode=\"r\", encoding='utf-8-sig') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line != \"\":\n",
        "                lines.append(line.strip())\n",
        "\n",
        "    return lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_Ha0PkT_GYid"
      },
      "outputs": [],
      "source": [
        "train = read_sentences_from_file('/content/drive/MyDrive/Colab Notebooks/train.txt')\n",
        "train_pd = pd.DataFrame(train,columns = ['text'])\n",
        "\n",
        "test = read_sentences_from_file('/content/drive/MyDrive/Colab Notebooks/test.txt')\n",
        "test_pd = pd.DataFrame(test,columns = ['text'])\n",
        "\n",
        "dev = read_sentences_from_file('/content/drive/MyDrive/Colab Notebooks/dev.txt')\n",
        "dev_pd = pd.DataFrame(dev,columns = ['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5_GhqZOOGYif"
      },
      "outputs": [],
      "source": [
        "#Tokenizing sentences\n",
        "\n",
        "train_pd['tok_text'] = train_pd.apply(lambda row: nltk.word_tokenize(row['text'].lower()), axis=1)\n",
        "dev_pd['tok_text'] = dev_pd.apply(lambda row: nltk.word_tokenize(row['text'].lower()), axis=1)\n",
        "test_pd['tok_text'] = test_pd.apply(lambda row: nltk.word_tokenize(row['text'].lower()), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9yFTk-CGYih"
      },
      "source": [
        "Extract a vocabulary consisting of the tokens that occur at least 3 times in the train set and output the size of your vocabulary.\n",
        "Also output the most frequent vocabulary token in the train set, which should be 'the'.\n",
        "Include the edge token, unknown token, and pad token in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BJdhcV_4GYih",
        "outputId": "192ec913-3167-4062-b7cf-f0fa5c9b20fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the 15548\n"
          ]
        }
      ],
      "source": [
        "word_counts = Counter(word for sent in train_pd['tok_text'] for word in sent)\n",
        "\n",
        "vocab = []\n",
        "most_freq_word_count = 0\n",
        "most_freq_word = \"\"\n",
        "for key in word_counts.keys():\n",
        "    if word_counts[key] >= 3:\n",
        "        vocab.append(key)\n",
        "    if  word_counts[key] > most_freq_word_count:\n",
        "        most_freq_word_count = word_counts[key]\n",
        "        most_freq_word = key\n",
        "\n",
        "    \n",
        "#vocab = [key for key in word_counts.keys() if word_counts[key] >= 3]\n",
        "\n",
        "vocab.append('<UNK>') #including unknown token\n",
        "vocab.append('<PAD>') #including pad token\n",
        "vocab.append('<EDGE>') #including pad token\n",
        "\n",
        "print(most_freq_word,most_freq_word_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DgJ6OBIGYij"
      },
      "source": [
        "Create a data set of indexified token sequences for the train set and dev set using the vocabulary created above in a way that is suitable for a language model, making use of edge tokens, unknown tokens, and pad tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iWg5DkY5GYij"
      },
      "outputs": [],
      "source": [
        "#<UNK>\n",
        "def apply_unk(sentence):\n",
        "    unkenized_sent = []\n",
        "    for word in sentence:\n",
        "        if word in vocab:\n",
        "            unkenized_sent.append(word)\n",
        "        else:\n",
        "            unkenized_sent.append(\"<UNK>\")\n",
        "    return unkenized_sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3FVRqfnRGYij"
      },
      "outputs": [],
      "source": [
        "train_pd['tok_text'] = train_pd['tok_text'].apply(apply_unk)\n",
        "test_pd['tok_text'] = test_pd['tok_text'].apply(apply_unk)\n",
        "dev_pd['tok_text'] = dev_pd['tok_text'].apply(apply_unk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9axZIe-kGYik"
      },
      "outputs": [],
      "source": [
        "train_texts = train_pd['tok_text'].tolist()\n",
        "test_texts = test_pd['tok_text'].tolist()\n",
        "dev_texts = dev_pd['tok_text'].tolist()\n",
        "vocab_dict = {k: v for v, k in enumerate(vocab)} #dict makes it more efficient\n",
        "\n",
        "train_text_lens = torch.tensor([len(sent) + 1 for sent in train_texts], dtype=torch.int64,device=device) # Include the edge token.\n",
        "test_text_lens = torch.tensor([len(sent) + 1 for sent in test_texts], dtype=torch.int64,device=device)\n",
        "dev_text_lens = torch.tensor([len(sent) + 1 for sent in dev_texts], dtype=torch.int64,device=device)\n",
        "\n",
        "train_max_len = max(train_text_lens)\n",
        "test_max_len = max(test_text_lens)\n",
        "dev_max_len = max(dev_text_lens)\n",
        "\n",
        "      \n",
        "padded_train_x = [['<EDGE>'] + sent + ['<PAD>']*(train_max_len - len(sent) - 1) for sent in train_texts]\n",
        "padded_test_x = [['<EDGE>'] + sent + ['<PAD>']*(test_max_len - len(sent) - 1) for sent in test_texts]\n",
        "padded_dev_x = [['<EDGE>'] + sent + ['<PAD>']*(dev_max_len - len(sent) - 1) for sent in dev_texts]\n",
        "\n",
        "\n",
        "padded_train_y = [sent + ['<EDGE>'] + ['<PAD>']*(train_max_len - len(sent) - 1) for sent in train_texts]\n",
        "padded_test_y = [sent + ['<EDGE>'] + ['<PAD>']*(test_max_len - len(sent) - 1) for sent in test_texts]\n",
        "padded_dev_y = [sent + ['<EDGE>'] + ['<PAD>']*(dev_max_len - len(sent) - 1) for sent in dev_texts]\n",
        "\n",
        "\n",
        "\n",
        "indexed_train_x = torch.tensor([[vocab_dict[token] for token in sent] for sent in padded_train_x], dtype=torch.int64,device=device)\n",
        "indexed_test_x = torch.tensor([[vocab_dict[token] for token in sent] for sent in padded_test_x], dtype=torch.int64,device=device)\n",
        "indexed_dev_x = torch.tensor([[vocab_dict[token] for token in sent] for sent in padded_test_x], dtype=torch.int64,device=device)\n",
        "\n",
        "\n",
        "indexed_train_y = torch.tensor([[vocab_dict[token] for token in sent] for sent in padded_train_y], dtype=torch.int64,device=device)\n",
        "indexed_test_y = torch.tensor([[vocab_dict[token] for token in sent] for sent in padded_test_y], dtype=torch.int64,device=device)\n",
        "indexed_dev_y = torch.tensor([[vocab_dict[token] for token in sent] for sent in padded_dev_y], dtype=torch.int64,device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQaf2Y41GYik"
      },
      "source": [
        "Finally, load the test set text file as single string and keep it in a variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PYc8DTi8GYil"
      },
      "outputs": [],
      "source": [
        "#test_pd['text'] += '\\n'\n",
        "\n",
        "#test_str = ' '.join(test_pd['text'].tolist())\n",
        "test_str = '\\n'.join(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeFnThSUGYil"
      },
      "source": [
        "## Evaluation tools (10%)\n",
        "\n",
        "We're going to need a function that evaluates our language models as well as a way to test that function before we make the language model.\n",
        "That second part is done by using a mock model which can be used exactly like a language model but that works with some simple and predictable rules.\n",
        "You can then use the mock model to check if the evaluation function works.\n",
        "\n",
        "In this assignment, a language model is defined as a PyTorch module whose `forward` method has the following signature:\n",
        "\n",
        "`def forward(self, x_indexes):`\n",
        "\n",
        "where\n",
        "\n",
        "* `x_indexes` is a PyTorch tensor giving the token indexes of a batch of sentences.\n",
        "    The tensor is of type `int64` with shape `(batch size, time steps)`.\n",
        "* The function returns a PyTorch tensor predicting the next token after each token in `x_indexes`.\n",
        "    This is done by returning a set of logits over the vocabulary for each token in `x_indexes`.\n",
        "    The tensor is of type `float32` with shape `(batch size, time steps, vocab size)`.\n",
        "    The last set of logits predicts the next token at the end of `x_indexes`.\n",
        "\n",
        "Do the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qWUJBqpGYil"
      },
      "source": [
        "Develop a mock language model.\n",
        "This language model will be a PyTorch module that always predicts the most frequent token in the train set as the next token, regardless of what the previous tokens were.\n",
        "Remember that it is logits that will be returned by the forward function, not probabilities.\n",
        "The name of this class should be `MockModel` and its initialiser should only take the vocabulary size as a parameter.\n",
        "\n",
        "Some test code has been provided to check that your mock model is correct.\n",
        "Fix the test code as instructed in the comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-DeKpfRYGYil"
      },
      "outputs": [],
      "source": [
        "class MockModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embedding_matrix = torch.nn.Parameter(torch.tensor(np.random.normal(0.0, 1.0, (vocab_size, 2)), dtype=torch.float32))\n",
        "        self.rnn_s0 = torch.nn.Parameter(torch.tensor(np.random.normal(0.0, 1.0, (2,)), dtype=torch.float32))\n",
        "        self.rnn = torch.nn.GRU(2, 2, batch_first=True, num_layers=1)\n",
        "        self.w = torch.nn.Parameter(torch.tensor(np.random.normal(0.0, 1.0, (2, vocab_size)), dtype=torch.float32))\n",
        "        self.b = torch.nn.Parameter(torch.zeros((vocab_size,), dtype=torch.float32))\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, x_indexes):\n",
        "        batch_size = x_indexes.shape[0]\n",
        "        timestamp = x_indexes.shape[1]\n",
        "        embedded = self.embedding_matrix[x_indexes]\n",
        "        \n",
        "        s0 = torch.stack((\n",
        "            self.rnn_s0,\n",
        "        ), dim=0)\n",
        "        s0 = s0.unsqueeze(1).tile((1, batch_size, 1))\n",
        "        (interm_states, _) = self.rnn(embedded, s0)\n",
        "        \n",
        "        logits = (interm_states@self.w + self.b).detach().numpy()\n",
        "\n",
        "        for i, row in enumerate(logits):\n",
        "            temp = [-1 for i in range(vocab_size)]\n",
        "            for j, r in enumerate(logits[i]):\n",
        "                temp[most_freq_index] = logits[i][j][most_freq_index]\n",
        "                logits[i][j] = temp\n",
        "        \n",
        "        return torch.tensor(logits, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CgPzszTdGYim",
        "outputId": "8c6d4f0f-328d-4d29-d4ab-24a034c39520",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct!\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(vocab) # Write the vocab size here.\n",
        "most_freq_index = vocab_dict[most_freq_word] # Write the vocab index of the most frequent token here.\n",
        "\n",
        "mock_model = MockModel(vocab_size)\n",
        "#mock_model.to(device)\n",
        "mock_x_indexes = torch.zeros((2, 3), dtype=torch.int64,device=device)\n",
        "mock_output = mock_model(mock_x_indexes)\n",
        "assert mock_output.shape == (2, 3, vocab_size), 'Output shape is invalid ({}).'.format(mock_output.shape)\n",
        "mock_probs = torch.softmax(mock_output, dim=2)\n",
        "assert (mock_probs.argmax(2) == most_freq_index).all(), 'Model is predicting tokens other than the most frequent token (these token indexes are being predicted: {}).'.format(set(mock_probs.argmax(2).flatten().tolist()))\n",
        "print('Correct!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bivmfm5RGYim"
      },
      "source": [
        "Next, we need a function that measures the perplexity of a language model on the dev set.\n",
        "Your function must take a model and data from the data set and return the perplexity over the entire data set.\n",
        "Don't forget that the perplexity includes the probability of the edge token at the end of the sentence.\n",
        "\n",
        "Use this function on the mock model and dev set and output the perplexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IR5tCqfXGYin"
      },
      "outputs": [],
      "source": [
        "def calc_perplexity(model, indexed_set_x, indexed_set_y,text_lens,texts):\n",
        "    with torch.no_grad():\n",
        "        outputs = torch.log_softmax(model(indexed_set_x), dim=2)\n",
        "        total_word_log_prob = 0.0\n",
        "        num_words = 0\n",
        "        for (sent, text_len, log_probs, y) in zip(texts, text_lens, outputs, indexed_set_y):\n",
        "            word_log_probs = log_probs[torch.arange(text_len), y[:text_len]].numpy()\n",
        "            total_word_log_prob += word_log_probs.sum()\n",
        "            num_words += text_len.numpy()\n",
        "        return np.exp(-1/num_words*total_word_log_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "__jdrlLAGYin"
      },
      "outputs": [],
      "source": [
        "#calc_perplexity(mock_model, indexed_dev_x, indexed_dev_y,dev_text_lens,dev_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfJCS_WtGYin"
      },
      "source": [
        "## Compression and decompression (20%)\n",
        "\n",
        "We will now write the code that makes the actual compression and decompression.\n",
        "Our compression algorithm will consist of checking if the next token is correctly guessed as the most probable token by the language model. \n",
        "If it is correctly guessed then we don't need to know it so we replace it with the single letter 'X', otherwise we leave the word there.\n",
        "Since all the text is in lowercase, there will never be an 'X' in it so we can safely use it as a flag.\n",
        "When decompressing, we will go through the text and replace all instances of 'X' with whatever the language model predicts is the most probable token in that place.\n",
        "\n",
        "Do the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwJ9g84SGYin"
      },
      "source": [
        "Start with the compression function.\n",
        "Write a function that takes in a language model and a text as parameters, where the text consists of sentences on separate lines and space separated tokens (just like the raw data sets).\n",
        "The function should return a single string with each line in the input text being compressed.\n",
        "The last line in the input text might be a blank line, in which case the compressed output must also end with a blank line.\n",
        "\n",
        "We want a compressed text to be decompressed back into the exact original text, which means that all out-of-vocabulary tokens must be left as is as if the language model did not predict them.\n",
        "\n",
        "Print out the result of compressing this sentence using the mock model:\n",
        "\n",
        "`el recodo was named after the town lizárraga was born in .`\n",
        "\n",
        "which should be compressed into \"el recodo was named after X town lizárraga was born in .\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "scrolled": true,
        "id": "ii30SHhvGYio"
      },
      "outputs": [],
      "source": [
        "def compress(model,text):\n",
        "    \n",
        "    tok_text = [line.split() for line in text.splitlines()]\n",
        "    compressed_text = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for sentence in tok_text:\n",
        "            \n",
        "            compressed_sentence_lst = []\n",
        "            prefixes = []\n",
        "\n",
        "            for i in range(len(sentence) +1):\n",
        "                prefixes.append(('<EDGE>',) + tuple(sentence[:i]))\n",
        "            \n",
        "            prediction = \"\"\n",
        "            \n",
        "            for prefix in prefixes:\n",
        "                \n",
        "                prefix_indexes = torch.tensor([[vocab_dict[token] if token in vocab else vocab_dict[\"<UNK>\"] for token in prefix]], dtype=torch.int64)\n",
        "                outputs = torch.softmax(model(prefix_indexes), dim=2)\n",
        "                prediction = vocab[outputs.argmax(2).numpy()[0][-1]]\n",
        "                \n",
        "                if prefix[-1] == prediction:\n",
        "                    \n",
        "                    compressed_sentence_lst.append(\"X\")\n",
        "\n",
        "                elif prefix[-1] != \"<EDGE>\":\n",
        "                    compressed_sentence_lst.append(prefix[-1])\n",
        "                \n",
        "                prefix_indexed = torch.tensor([[vocab_dict[token] if token in vocab else vocab_dict['<UNK>'] for token in prefix]], dtype=torch.int64)\n",
        "                outputs = torch.softmax(model(prefix_indexed), dim=2)\n",
        "                \n",
        "                prediction = vocab[outputs.argmax(2).numpy()[0][-1]]\n",
        "            compressed_text.append(' '.join(compressed_sentence_lst))\n",
        "            \n",
        "            return '\\n'.join(compressed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rRaX8zGdGYio",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bac67343-0ce4-4537-dabb-fc375c8a215d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'el recodo was named after X town lizárraga was born in .'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "text = \"el recodo was named after the town lizárraga was born in .\"\n",
        "compress(mock_model,text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zax5-jJGYip"
      },
      "source": [
        "Now write the decompression function.\n",
        "The function should take in a language model and a compressed text as parameters (as well as any other extra information needed), where the compressed text consists of sentences on separate lines and space separated tokens (just like the raw data sets) but with some tokens having been replaced by 'X'.\n",
        "The function should return a single big string.\n",
        "Each line in the compressed text should be decompressed back into the original input line.\n",
        "The last line in the compressed text might be a blank line, in which case the decompressed output must also end with a blank line.\n",
        "\n",
        "Note that the original token that was replaced by an 'X' is to be found by using the language model to predict what comes after the sentence prefix preceeding the 'X'.\n",
        "Also note that the predicted token must replace the 'X' in the sentence before another prediction is made.\n",
        "\n",
        "Print out the result of decompressing the compressed text:\n",
        "\n",
        "`el recodo was named after X town lizárraga was born in .`\n",
        "\n",
        "which should be decompressed into \"el recodo was named after the town lizárraga was born in .\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KRVsgOj6GYip"
      },
      "outputs": [],
      "source": [
        "def decompress(model,text):\n",
        "    \n",
        "    tok_text = [line.split() for line in text.splitlines()]\n",
        "    decompressed_text = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for sentence in tok_text:\n",
        "            \n",
        "            decompressed_sentence_lst = []\n",
        "            prefixes = []\n",
        "            \n",
        "            for i in range(len(sentence) + 1):\n",
        "                prefixes.append(['<EDGE>'] + sentence[:i])\n",
        "            \n",
        "            prediction = \"\"\n",
        "            \n",
        "            replaced_words = []\n",
        "            \n",
        "            for index, prefix in enumerate(prefixes):\n",
        "                \n",
        "                if replaced_words: #if replaced_words is not empty\n",
        "\n",
        "                    c = 0\n",
        "                    for i, elem in enumerate(prefix):\n",
        "                        if elem == \"X\":\n",
        "                            prefix[i] = replaced_words[c]\n",
        "                            c+=1\n",
        "                        if c == len(replaced_words):\n",
        "                            break\n",
        "                \n",
        "                if prefix[-1] == \"X\":\n",
        "                    #if last word in prefix is X\n",
        "                    replaced_words.append(prediction)\n",
        "                    decompressed_sentence_lst.append(prediction)\n",
        "                    prefix.pop()\n",
        "                    prefix.append(prediction)\n",
        "                    \n",
        "                elif prefix[-1] != \"<EDGE>\": #if not X or <EDGE>\n",
        "                    decompressed_sentence_lst.append(prefix[-1])\n",
        "                \n",
        "                prefix_indexed = torch.tensor([[vocab.index(token) if token in vocab else vocab.index(\"<UNK>\") for token in prefix]], dtype=torch.int64)\n",
        "              \n",
        "                outputs = torch.softmax(model(prefix_indexed), dim=2)\n",
        "\n",
        "                prediction = vocab[outputs.argmax(2).numpy()[0][-1]]\n",
        "\n",
        "            decompressed_text.append(' '.join(decompressed_sentence_lst))\n",
        "        \n",
        "        return '\\n'.join(decompressed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DOjVYFyGGYip",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d421ee62-e775-401e-9552-5586cffbcd2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'el recodo was named after the town lizárraga was born in .'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "compressed_text = \"el recodo was named after X town lizárraga was born in .\"\n",
        "decompress(mock_model,compressed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9D41KDmGYip"
      },
      "source": [
        "Next, calculate and print the space saving amount of the mock model on the test set.\n",
        "The space saving amount is calculated as follows:\n",
        "\n",
        "$$\\text{space\\_saving}(t) = 1 - \\frac{|\\text{compress}(t)|}{|t|}$$\n",
        "\n",
        "where $|t|$ is the number of characters in text $t$.\n",
        "\n",
        "This measure tells you what fraction of the original size has been shaved off after compression (higher is better)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IC0JCy7yGYiq"
      },
      "outputs": [],
      "source": [
        "def space_saved(model,text):\n",
        "    compressed_txt = compress(model,text)\n",
        "    \n",
        "    return 1- (len(compressed_txt)/len(text))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zpm38Me4GYiq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac14ede-8b95-4090-957f-911428ccc190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text el recodo was named after the town lizárraga was born in .\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03448275862068961"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "print(\"Text\",text)\n",
        "space_saved(mock_model,text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGreZudcGYiq"
      },
      "source": [
        "## Making and using a language model (40%)\n",
        "\n",
        "Now we finally train a language model and use it to compress the test set.\n",
        "\n",
        "Do the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXe0MY1TGYiq"
      },
      "source": [
        "Train a neural language model on the train set.\n",
        "After training, plot a graph of how the dev set perplexity varies with each epoch (use the perplexity function you wrote above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SRSJbwCGYiq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2173415b-0e14-4700-e823-c56cd83c42f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step error epoch_perp\n"
          ]
        }
      ],
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding_matrix = torch.nn.Parameter(torch.tensor(np.random.normal(0.0, 1.0, (vocab_size, embedding_size)), dtype=torch.float32, device = device))\n",
        "        self.rnn_s0 = torch.nn.Parameter(torch.tensor(np.random.normal(0.0, 1.0, (hidden_size,)), dtype=torch.float32, device = device))\n",
        "        self.rnn = torch.nn.GRU(embedding_size, hidden_size, batch_first=True, num_layers=1)\n",
        "        self.w = torch.nn.Parameter(torch.tensor(np.random.normal(0.0, 1.0, (hidden_size, vocab_size)), dtype=torch.float32, device = device))\n",
        "        self.b = torch.nn.Parameter(torch.zeros((vocab_size,), dtype=torch.float32, device = device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        embedded = self.embedding_matrix[x]\n",
        "\n",
        "        s0 = torch.stack((\n",
        "            self.rnn_s0,\n",
        "        ), dim=0)\n",
        "        s0 = s0.unsqueeze(1).tile((1, batch_size, 1))\n",
        "        (interm_states, _) = self.rnn(embedded, s0)\n",
        "\n",
        "        return interm_states@self.w + self.b\n",
        "    \n",
        "model = Model(len(vocab), embedding_size=128, hidden_size=64)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "\n",
        "batch_size = indexed_train_x.shape[0]\n",
        "time_steps = indexed_train_x.shape[1]\n",
        "mask = torch.zeros((batch_size, time_steps), dtype=torch.float32)\n",
        "\n",
        "perplexities = []\n",
        "\n",
        "print('step', 'error','epoch_perp')\n",
        "for step in range(1, 100+1):\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        mask[i, :train_text_lens[i]] = 1.0\n",
        "    \n",
        "    optimiser.zero_grad()\n",
        "    output = model(indexed_train_x)\n",
        "    print('indexed_train_y shape',indexed_train_y.shape)\n",
        "    print('output.transpose(1, 2) shape',output.transpose(1, 2).shape)\n",
        "    errors = torch.nn.functional.cross_entropy(output.transpose(1, 2), indexed_train_y, reduction='none')\n",
        "    errors *= mask\n",
        "    error = errors.sum()/train_text_lens.sum()\n",
        "    error.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    epoch_perp = calc_perplexity(model, indexed_dev_x, indexed_dev_y,dev_text_lens,dev_texts)\n",
        "    perplexities.append(epoch_perp)\n",
        "    \n",
        "\n",
        "    if step%1 == 0:\n",
        "        print(step, error.detach().tolist(),epoch_perp)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "#batch_size = indexed_train_x.shape[0]\n",
        "time_steps = indexed_train_x.shape[1]\n",
        "mask = torch.zeros((batch_size, time_steps), dtype=torch.float32,device = device)\n",
        "\n",
        "perplexities = []\n",
        "\n",
        "\n",
        "print('step', 'error','epoch_perp')\n",
        "for step in range(1, 100+1):\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        mask[i, :train_text_lens[i]] = 1.0\n",
        "    \n",
        "    \n",
        "    num_minibatches = int(np.ceil(len(indexed_train_x)/minibatch_size))\n",
        "    print(\"num_minibatches\",num_minibatches)\n",
        "\n",
        "    indexes = np.arange(len(indexed_train_x))\n",
        "\n",
        "    for i in range(num_minibatches):\n",
        "        minibatch_indexes = indexes[i*minibatch_size:(i+1)*minibatch_size]\n",
        "        minibatch_x = torch.tensor(indexed_train_x[minibatch_indexes], dtype=torch.int64, device=device)\n",
        "        minibatch_y = torch.tensor(indexed_train_y[minibatch_indexes], dtype=torch.int64, device=device)\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "        output = model(minibatch_x)\n",
        "        errors = torch.nn.functional.cross_entropy(output.transpose(1, 2), minibatch_y, reduction='none')\n",
        "        errors *= mask\n",
        "        error = errors.sum()/train_text_lens.sum()\n",
        "        error.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "^bil mini batches\n",
        "\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gMVbwYKn2iPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1,len(perplexities)+1),perplexities)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Grfac7ZaRE-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVh-stLvGYir"
      },
      "source": [
        "Now measure the space saving amount of the trained model on the test text.\n",
        "Also check that when you decompress the compressed test text you get exactly the same string as the test text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuGghaaSGYir"
      },
      "outputs": [],
      "source": [
        "print(space_saved(model,test_str))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcvDZLwqGYir"
      },
      "source": [
        "Now we'll analyse the model a bit.\n",
        "Split the test text into sentences and compress each individual sentence.\n",
        "Print out the top 5 most compressed sentences and the top 5 least compressed sentences according to the space saving metric together with the compressed sentences."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = test_str.split('\\n')"
      ],
      "metadata": {
        "id": "rUn4leLwSVaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT9UiRNCGYir"
      },
      "outputs": [],
      "source": [
        "space_saved_test = {}\n",
        "\n",
        "for test_sent in test_sentences[:50]:\n",
        "\n",
        "  compressed_sent = compress(model,test_sent)\n",
        "  space_saved_test[compressed_sent] = space_saved(model,test_sent)\n",
        "\n",
        "sort_space_saved_test = sorted(space_saved_test.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Top 5 compressed\")\n",
        "display(sort_space_saved_test[:5])\n",
        "\n",
        "print(\"Least 5 compressed\")\n",
        "display(sorted(sort_space_saved_test[-5:], key=lambda x: x[1], reverse=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGuDJlhIGYir"
      },
      "source": [
        "Can we explain this difference in compression using sentences' similarities to the train set?\n",
        "Extract all the trigrams from each of the test sentences and, for each sentence, count how many of its trigrams are also found in the train set.\n",
        "Divide this count by the number of trigrams in the sentence in order to have a domain similarity measure.\n",
        "In order for the fraction to be meaningful from the language model's point of view, the edge token must be added to the front of the test sentences and out-of-vocabulary tokens must be replaced with the unknown token.\n",
        "Plot a scatter plot showing how this domain similarity measure relates to the space saving amount of each test sentence."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_ngrams(tokenized_sent, num):\n",
        "    n_grams = nltk.ngrams(tokenized_sent, num)\n",
        "    return [ ' '.join(grams) for grams in n_grams]"
      ],
      "metadata": {
        "id": "6xNGELDDc6c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpykrR9KGYis"
      },
      "outputs": [],
      "source": [
        "test_trigrams = []\n",
        "for tok_test_sentence in test_pd['tok_text']:\n",
        "  test_trigrams.append(extract_ngrams(tok_test_sentence,3))\n",
        "\n",
        "train_trigrams = []\n",
        "for tok_train_sentence in train_pd['tok_text']:\n",
        "  train_trigrams.append(extract_ngrams(tok_train_sentence,3))\n",
        "\n",
        "print(test_pd['tok_text'] == train_pd['tok_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLeRSZEoGYis"
      },
      "source": [
        "If the domain similarity measure we used was enough for explaining the compressibility, the above plot would look a bit like a straight line.\n",
        "Why is domain similarity not enough for explaining the compressability?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EIBYsO4GYis"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_9Wbzl6GYis"
      },
      "source": [
        "## Conclusions (10%)\n",
        "\n",
        "Write the following conclusions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah1bh84YGYis"
      },
      "source": [
        "What is a simple change in the compression algorithm that can be made to increase compression?\n",
        "Do not suggest any fundamental changes; the algorithm must still work by predicting missing tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COWedbl1GYis"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9spEjRhGYis"
      },
      "source": [
        "Write, in less than 300 words, your interpretation of the results and how you think the model could perform better.\n",
        "You should talk about things like overfitting/underfitting and whether the model is learning anything deep about English sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do3uhAJYGYit"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "75ee2b71ad44bf9ef4e9bee896f68ffbc764a6a2c6d1f57c86c48f99ffc25ca8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Colab_text_compression (07/01/22).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}